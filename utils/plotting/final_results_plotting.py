# -*- coding: utf-8 -*-
"""final_results_plotting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rg-WdmRs-qoRVm2Qw3widVC-N734i6mS
"""

import os

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from utils.misc import create_directory

# Changing plotting parameters
# - https://stackoverflow.com/questions/12444716/how-do-i-set-the-figure-title-and-axes-labels-font-size-in-matplotlib
# ----------------------------
import matplotlib.pylab as pylab
from matplotlib.ticker import StrMethodFormatter

# font_size = 16  # 'xx-large'
# params = {'legend.fontsize': font_size,
#           'figure.figsize': (15, 6),
#           'axes.labelsize': font_size,
#           'axes.titlesize': font_size,
#           'xtick.labelsize': font_size,
#           'ytick.labelsize': font_size}
# pylab.rcParams.update(params)
pylab.rcParams['axes.linewidth'] = 3.5  # set the value globally
# plt.rcParams['axes.xmargin'] = 0

def export_legend(legend, filename, expand=[-5,-5,5,5]):
    fig  = legend.figure
    fig.canvas.draw()
    bbox  = legend.get_window_extent()
    bbox = bbox.from_extents(*(bbox.extents + np.array(expand)))
    bbox = bbox.transformed(fig.dpi_scale_trans.inverted())
    fig.savefig(filename, dpi="figure", bbox_inches=bbox)

def set_box_color(bp, color):
    plt.setp(bp['boxes'], color=color)
    plt.setp(bp['whiskers'], color=color)
    plt.setp(bp['caps'], color=color)
    plt.setp(bp['medians'], color=color)
    plt.setp(bp['fliers'], markeredgecolor=color)

def improve_data(data, mode):
    if mode == " min":
        data = data - data * 0.05
    else:
        data = data + data * 0.05
    return data

"""# CENTRALIZED_VS_FEDERATED"""

SCALE_FACTORS = [2, 3, 6] 
NORM_METHOD = "square_root"
EPOCHS = 30

CSV_BASE_PATH = f"../evaluation_results/centralized_vs_federated/"

def extract_data(scale_factors, norm_method, epochs, clients_range, csv_base_path):
    """
    Return
    ---------
    - all_metrices{
        "x2": [bicubic-->[MAE, PSNR, SSIM], centralized-->[MAE, PSNR, SSIM], federated_2-->[MAE, PSNR, SSIM], ..., federated_20-->[MAE, PSNR, SSIM]],
        "x3": [bicubic-->[MAE, PSNR, SSIM], centralized-->[MAE, PSNR, SSIM], federated_2-->[MAE, PSNR, SSIM], ..., federated_20-->[MAE, PSNR, SSIM]],
        "x6": [bicubic-->[MAE, PSNR, SSIM], centralized-->[MAE, PSNR, SSIM], federated_2-->[MAE, PSNR, SSIM], ..., federated_20-->[MAE, PSNR, SSIM]]
    }
    """
    # EXTRACTING DATA
    
    all_metrics = {}
    
    for s in scale_factors:
        metric_comparison = []
        
        #df_b = pd.read_csv(os.path.join(csv_base_path, f"x{s}", f"bicubic-epochs{epochs}-{norm_method}-x{s}.csv"))
        #metric_comparison.append([df_b["mae_original"],df_b["psnr_original"],df_b["ssim_original"]])
        
        df_c = pd.read_csv(os.path.join(csv_base_path, f"x{s}", f"centralized-epochs{epochs}-{norm_method}-x{s}.csv"))
        metric_comparison.append([df_c["mae_original"],df_c["psnr_original"],df_c["ssim_original"]])
        
        for c in clients_range:
            df_f = pd.read_csv(os.path.join(csv_base_path, f"x{s}", f"federated-performance-clients{c}-epochs{epochs}-{norm_method}-x{s}.csv"))
            metric_comparison.append([df_f["mae_original"],df_f["psnr_original"],df_f["ssim_original"]])
        
        all_metrics[f"x{s}"] = np.array(metric_comparison)
    
    return all_metrics
            
all_metrics = extract_data(SCALE_FACTORS, NORM_METHOD, EPOCHS, range(2,21), CSV_BASE_PATH)
all_metrics_no_bicubic = {s: all_metrics[s][1:] for s in all_metrics.keys()}

print(all_metrics["x2"].shape)
print(all_metrics["x3"].shape)
print(all_metrics["x6"].shape)

print(np.mean(all_metrics["x2"][0,0]))
print(np.mean(all_metrics["x3"][0,0]))
print(np.mean(all_metrics["x6"][0,0]))

print(np.mean(all_metrics_no_bicubic["x2"][0,0]))
print(np.mean(all_metrics_no_bicubic["x3"][0,0]))
print(np.mean(all_metrics_no_bicubic["x6"][0,0]))

def autolabel(ax, rects, mode, scaling_height=1, scaling_bar_text=1, metric=None):
    """
    Attach a text label above each bar displaying its height
    """
    
    all_heights = [rect.get_height() for rect in rects]
    fontsize = 25
    
    if mode =="min":
        best_val = np.min(all_heights)
    elif mode =="max":
        best_val = np.max(all_heights)
    else:
        raise ValueError('Vale of `mode` parameter not valid.')
                
        
    for i, rect in enumerate(rects):
        height = rect.get_height()
        hp = rect.get_x() + rect.get_width()/scaling_bar_text # horizontal position of the text
        fontweight= "bold" if height == best_val else None
        if metric == 2:
            ax.text(
                hp, 
                scaling_height*height,
                "{:.4f}".format(height),
                ha='center', 
                va='bottom',
                rotation=90,
                fontsize=fontsize,
                fontweight=fontweight
            )
        else:
            ax.text(
                hp, 
                scaling_height*height,
                "{:.3f}".format(height),
                ha='center', 
                va='bottom',
                rotation=90,
                fontsize=fontsize,
                fontweight=fontweight
            )


def plot_metric_comparison(all_metrics, save_base_path=None, show_legend=True):
    """Plotting bar for MAE, PSNR, SSIM comparisons for centralized vs federated"""
    if save_base_path:
        create_directory(save_base_path)
    
    # Changing plotting parameters
    font_size = 35  # 'xx-large'
    if show_legend:
        params = {'font.size': font_size, 'figure.figsize': (20, 11), 'legend.fontsize': font_size}
    else:
        params = {'font.size': font_size, 'figure.figsize': (20, 6), 'legend.fontsize': font_size}
    pylab.rcParams.update(params)
    
    # PLOTTING
    ind = np.arange(next(iter(all_metrics.values())).shape[0])*1.05 # the x locations for the groups
    width = 0.3       # the width of the bars
#     colors = ['#00cc99', '#0066ff', '#ff9933'] # bars colors
    colors = ['#0066ff', '#00cc99', '#ff9933'] # bars colors    
    metrics = ["MAE (Mbit)", "PSNR (dB)", "SSIM"]
    
    ymins = [0, 32, 0.80]
    ymaxs = [11.8, 80, 1.13]
    scaling_height = [1.03, 1.01, 1.005]
    modes = ["min","max","max"]
    scaling_bar_text = [4, 2, 1.5] # horizontal positions
    
    for i in range(3):

        """if i==1:
            fig, ax = plt.subplots(figsize=(20,20))
        else:
            fig, ax = plt.subplots()"""

        fig, ax = plt.subplots()
        if i==1:
            ax.margins(0.01, 0.1)
        else:
            ax.margins(0.01, 0) # to pad the bars limits with 2% of the data range in the x-direction and 0% in the y-direction --> avoid white space before and after
        
        offset = 0
        
        all_rects = []
        
        for k, s in enumerate(all_metrics.keys()):
            bar_values = np.mean(all_metrics[s], axis=(-1))[:, i]
            if i==0:
                bar_values/=1024
            all_rects.append(ax.bar(ind + offset, bar_values, width, color=colors[k]))
            offset += width

        # add some text for labels, title and axes ticks
        ax.set_ylabel(metrics[i], labelpad=30)
        ax.set_xlabel("# Clients", labelpad=20)
#         ax.set_title('Scores by group and gender')
        ax.set_xticks(ind + width)
        ax.set_xticklabels(np.arange(1, len(bar_values)+1))

        if show_legend:
            ax.legend(all_rects, all_metrics.keys(), ncol=3, loc='lower center', bbox_to_anchor=(0.5, -0.75))
        
#         plt.ylim(ymin=ymins[i], ymax=ax.get_ylim()[1]*scaling_height[i])
        plt.ylim(ymin=ymins[i], ymax=ymaxs[i])
        
        for k, rects_container in enumerate(all_rects):
            autolabel(ax, rects_container, modes[i], scaling_height[i], scaling_bar_text[k], metric=i)
            
#         plt.grid(linewidth=2,axis='y') # show only vertical grid lines

        #fig.tight_layout()
        plt.tight_layout()
        
        if save_base_path:
            output_path = os.path.join(save_base_path, f"{metrics[i]}_bar_average" + ("_legend" if show_legend else '') + ".png")
            plt.savefig(output_path)
        
        plt.show()

save_base_path = f"./plots/evaluation_results/centralized_vs_federated"
plot_metric_comparison(all_metrics_no_bicubic, save_base_path)
plot_metric_comparison(all_metrics_no_bicubic, save_base_path, show_legend=False)

def plot_federated_performance_losses(all_metrics, save_base_path=None, show_legend=True):
    # Changing plotting parameters
    font_size = 38  # 'xx-large'
    params = {'font.size': font_size, 'figure.figsize': (19, 8), 'legend.fontsize': font_size}
    pylab.rcParams.update(params)
    
    # PLOTTING
    ind = np.arange(next(iter(all_metrics.values())).shape[0]) # the x locations for the groups
    width = 0.3       # the width of the bars
    colors = ['#0066ff', '#00cc99', '#ff9933'] # bars colors
    metrics = ["MAE (Mbit)", "PSNR (dB)", "SSIM"]
    
    ymins = [0.05, 0.5, 0.002]
    ymaxs = [11.8, 62, 1.13]
    yperc = [0.01, 0.01, 0.0005]
    scaling_height = [1.03, 1.01, 1.005]
    modes = ["min","max","max"]
    scaling_bar_text = [4, 2, 1.5] # horizontal positions
    
    for i in range(3):
        for k, s in enumerate(all_metrics.keys()):
            
            fig, ax = plt.subplots()
            
            ax.margins(0.02, 0) # to pad the bars limits with 2% of the data range in the x-direction and 0% in the y-direction --> avoid white space before and after
            
            bar_values = np.mean(all_metrics[s], axis=(-1))[:, i]
            if i==0:
                bar_values/=1024 # MAE in Mbit
            max_num_clients = len(bar_values)

            ax.plot(bar_values, marker="o", linestyle='--', color="r", linewidth=3.5, markersize=8)
            bars = ax.bar(range(max_num_clients), bar_values,color=colors[k])
            if s == "x6":
                yperc[i] *=4 
            plt.ylim(bottom=np.min(bar_values)*(1-yperc[i]), top=np.max(bar_values)*(1+yperc[i]))
            plt.xlabel('# Clients', labelpad=20)
            plt.ylabel(metrics[i], labelpad=20)
            plt.xticks(range(max_num_clients), range(1, max_num_clients + 1))
            
            if modes[i] == "min":
                index_best = np.argmin(bar_values)
            else:
                index_best = np.argmax(bar_values)
            print("{:15s} {:.6f}".format("Min val_loss:", bar_values[index_best]))
            print("{:15s} {}".format("N. Clients:", index_best + 1))
            
            if show_legend:
                if i > 0:
                    ax.legend([bars],[s], loc="upper left")
                else:
                    ax.legend([bars],[s])
            
            plt.tight_layout()
            if save_base_path:
                output_path = os.path.join(save_base_path, f"{metrics[i]}_bar_average-performance_focus-{s}" + ("_legend" if show_legend else '') + ".png")
                plt.savefig(output_path)
                print(output_path)
            plt.show()
            
save_base_path = f"./plots/evaluation_results/centralized_vs_federated"
plot_federated_performance_losses(all_metrics_no_bicubic, save_base_path)

"""# Plot with variability in clients partecipation"""

def extract_data_clients_rnd(scale_factors, norm_method, epochs, num_clients, percentage_clients, csv_base_path):
    """
    Return
    ---------
    - all_metrices{
        "x2": [10%-->[MAE, PSNR, SSIM], 20%-->[MAE, PSNR, SSIM], 30%-->[MAE, PSNR, SSIM], ..., 100%-->[MAE, PSNR, SSIM]],
        "x3": [10%-->[MAE, PSNR, SSIM], 20%-->[MAE, PSNR, SSIM], 30%-->[MAE, PSNR, SSIM], ..., 100%-->[MAE, PSNR, SSIM]],
        "x6": [10%-->[MAE, PSNR, SSIM], 20%-->[MAE, PSNR, SSIM], 30%-->[MAE, PSNR, SSIM], ..., 100%-->[MAE, PSNR, SSIM]],
    }
    """
    # EXTRACTING DATA
    
    all_metrics = {}
    
    for s in scale_factors:
        metric_comparison = []    
        for p in percentage_clients:
            percentage_str = str("{:.2f}".format(p*100.)).replace('.','_')
            df_p = pd.read_csv(os.path.join(csv_base_path, f"x{s}", f"federated-performance-clients{num_clients}-percentage_clients{percentage_str}-epochs{epochs}-{norm_method}-x{s}.csv"))
            metric_comparison.append([df_p["mae_original"],df_p["psnr_original"],df_p["ssim_original"]])
        
        all_metrics[f"x{s}"] = np.array(metric_comparison)
    
    return all_metrics

all_metrics_clients_rnd = extract_data_clients_rnd(SCALE_FACTORS, NORM_METHOD, EPOCHS, 10, np.arange(1, 11, 1)/10, CSV_BASE_PATH)

print(all_metrics_clients_rnd["x2"].shape)

print(np.mean(all_metrics_clients_rnd["x2"][0, 0]))
print(np.mean(all_metrics_clients_rnd["x3"][0, 0]))
print(np.mean(all_metrics_clients_rnd["x6"][0, 0]))

print(np.mean(all_metrics_clients_rnd["x2"], axis=(-1))[:, 0])
print(np.mean(all_metrics_clients_rnd["x3"], axis=(-1))[:, 0])
print(np.mean(all_metrics_clients_rnd["x6"], axis=(-1))[:, 0])

print(np.std(all_metrics_clients_rnd["x2"], axis=(-1))[:, 0])
print(np.std(all_metrics_clients_rnd["x3"], axis=(-1))[:, 0])
print(np.std(all_metrics_clients_rnd["x6"], axis=(-1))[:, 0])

def autolabel(ax, rects, mode, scaling_height=1, scaling_bar_text=1, metric=None):
    """
    Attach a text label above each bar displaying its height
    """
    
    all_heights = [rect.get_height() for rect in rects]
    fontsize = 34
    
    if mode =="min":
        best_val = np.min(all_heights)
    elif mode =="max":
        best_val = np.max(all_heights)
    else:
        raise ValueError('Vale of `mode` parameter not valid.')
                
        
    for i, rect in enumerate(rects):
        height = rect.get_height()
        hp = rect.get_x() + rect.get_width()/scaling_bar_text # horizontal position of the text
        fontweight= "bold" if height == best_val else None
        if metric == 2:
            ax.text(
                hp, 
                scaling_height*height,
                "{:.4f}".format(height),
                ha='center', 
                va='bottom',
                rotation=90,
                fontsize=fontsize,
                fontweight=fontweight
            )
        else:
            ax.text(
                hp, 
                scaling_height*height,
                "{:.3f}".format(height),
                ha='center', 
                va='bottom',
                rotation=90,
                fontsize=fontsize,
                fontweight=fontweight
            )

def plot_metric_comparison(all_metrics, save_base_path=None, show_legend=True, show_std=False):
    """Plotting bar for MAE, PSNR, SSIM comparisons for centralized vs federated"""
    if save_base_path:
        create_directory(save_base_path)
    
    # Changing plotting parameters
    font_size = 35  # 'xx-large'
    if show_legend:
        params = {'font.size': font_size, 'figure.figsize': (20, 10), 'legend.fontsize': font_size}
    else:
        params = {'font.size': font_size, 'figure.figsize': (20, 7), 'legend.fontsize': font_size}
    pylab.rcParams.update(params)
    
    # PLOTTING
    ind = np.arange(next(iter(all_metrics.values())).shape[0])*1.1 # the x locations for the groups
    width = 0.3       # the width of the bars
#     colors = ['#00cc99', '#0066ff', '#ff9933'] # bars colors
    colors = ['#0066ff', '#00cc99', '#ff9933'] # bars colors
    yerr_colors = ['#002761', '#0e4a3b', '#7a3b00'] # bars colors    
    metrics = ["MAE (Mbit)", "PSNR (dB)", "SSIM"]
    
    ymins = [0, 32, 0.80]
    ymins_std = [0, 25, 0.5]
    
    ymaxs = [13, 62, 1.13]
    ymaxs_std=[15, 55, 1.13]
    
    scaling_height = [1.05, 1.01, 1.005]
    modes = ["min","max","max"]
    scaling_bar_text = [2, 2, 2] # horizontal positions
    
    for i in range(3):
        fig, ax = plt.subplots()
        
#         plt.xlim(left=-0.4, right=ind.shape[0])
        ax.margins(0.02, 0) # to pad the bars limits with 2% of the data range in the x-direction and 0% in the y-direction --> avoid white space before and after

        
        offset = 0
        
        all_rects = []
        
        for k, s in enumerate(all_metrics.keys()):
            bar_values = np.mean(all_metrics[s], axis=(-1))[:, i]
            bar_std = np.std(all_metrics[s], axis=(-1))[:, i]
            if i==0:
                bar_values/=1024
                bar_std/=1024
            if show_std:
                all_rects.append(ax.bar(ind + offset, bar_values, width, color=colors[k], yerr=bar_std, error_kw=dict(ecolor=yerr_colors[k], lw=4, capsize=6, capthick=2)
                                       ))
            else:
                all_rects.append(ax.bar(ind + offset, bar_values, width, color=colors[k]))
            offset += width

        # add some text for labels, title and axes ticks
        ax.set_ylabel(metrics[i], labelpad=30)
        ax.set_xlabel("Clients per Round (%)", labelpad=20)
#         ax.set_title('Scores by group and gender')
        ax.set_xticks(ind + width)
        ax.set_xticklabels([f"{x * 10}" for x in np.arange(1,len(bar_values)+1)])

        if show_legend:
            ax.legend(all_rects, all_metrics.keys(), ncol=3, loc='lower center',bbox_to_anchor=(0.5, -0.6))
        
#         plt.ylim(ymin=ymins[i], ymax=ax.get_ylim()[1]*scaling_height[i])
        if not show_std:
            plt.ylim(ymin=ymins[i], ymax=ymaxs[i])
        else:
            plt.ylim(ymin=ymins_std[i], ymax=ymaxs_std[i])
            
        
        if not show_std:
            for k, rects_container in enumerate(all_rects):
                autolabel(ax, rects_container, modes[i], scaling_height[i], scaling_bar_text[k], metric=i)
        
        plt.tight_layout()
        
        if save_base_path:
            output_path = os.path.join(save_base_path, f"{metrics[i]}_bar_average"+ ("_std" if show_std else '') + ("_legend" if show_legend else '') + ".png")
            plt.savefig(output_path)
        
        plt.show()

save_base_path = f"./plots/evaluation_results/clients_partecipation_variability"
plot_metric_comparison(all_metrics_clients_rnd, save_base_path)
plot_metric_comparison(all_metrics_clients_rnd, save_base_path, show_legend=False)
plot_metric_comparison(all_metrics_clients_rnd, save_base_path, show_legend=False, show_std =True)
plot_metric_comparison(all_metrics_clients_rnd, save_base_path, show_legend=True, show_std =True)

"""# FEDERATED PRIVACY"""

TM_ORIGINAL_SIZE = 23
SUB_MATRICES_SIZE_RANGE = np.arange(11,24)

def plot_overlapping_perc(tm_original_size, sub_matrices_size_range, save_base_path=None):
    if save_base_path:
        create_directory(save_base_path)
    font_size = 26  # 'xx-large'
    params = {'font.size': font_size, 'figure.figsize': (13, 10), 'legend.fontsize': font_size, 'legend.markerscale': 1.5}
    pylab.rcParams.update(params)

    
    overlapping = np.array([1-((tm_original_size-(sub*2-tm_original_size))**2)/tm_original_size**2 for sub in sub_matrices_size_range]).clip(min=0)
    print(overlapping)
    ind = np.arange(sub_matrices_size_range[-1]- sub_matrices_size_range[0]+1)
    
    plt.plot(overlapping, marker="o", linestyle='-', color="b",linewidth=3,markersize=8, label="Submatrices overlap (%)")
#     plt.bar(ind, overlapping, color="#00a9ff")
    plt.xlabel('Submatrix Size', labelpad=20)
    plt.ylabel('Overlap (%)')
    plt.xticks(ticks=ind, labels=sub_matrices_size_range)
    plt.yticks(ticks=overlapping, labels=["{:d}".format(int(round(i))) if i <=95 or i ==100 else "" for i in overlapping * 100])
#     plt.grid(linewidth=2.5)
#     plt.grid(linewidth=2, axis='y')
#     plt.gca().set_aspect('equal', adjustable='box') #  makes x and y have the same scale
    plt.legend()
    
    plt.tight_layout()
    
    if save_base_path:
            output_path = os.path.join(save_base_path, f"overlapping_percentage.png")
            plt.savefig(output_path)
    plt.show()
    return overlapping

save_base_path = f"./plots/evaluation_results/federated_privacy"
overlapping = plot_overlapping_perc(TM_ORIGINAL_SIZE, SUB_MATRICES_SIZE_RANGE, save_base_path)

def extract_data_clients_privacy(scale_factors, norm_method, epochs, num_clients, sub_matrices_range, csv_base_path):
    """
    Return
    ---------
    - all_metrices{
        "x2": [10%-->[MAE, PSNR, SSIM], 20%-->[MAE, PSNR, SSIM], 30%-->[MAE, PSNR, SSIM], ..., 100%-->[MAE, PSNR, SSIM]],
        "x3": [10%-->[MAE, PSNR, SSIM], 20%-->[MAE, PSNR, SSIM], 30%-->[MAE, PSNR, SSIM], ..., 100%-->[MAE, PSNR, SSIM]],
        "x6": [10%-->[MAE, PSNR, SSIM], 20%-->[MAE, PSNR, SSIM], 30%-->[MAE, PSNR, SSIM], ..., 100%-->[MAE, PSNR, SSIM]],
    }
    """
    # EXTRACTING DATA
    
    all_metrics = {}
    
    for s in scale_factors:
        metric_comparison = []    
        for SUB_MATRICES_SIZE in sub_matrices_range:
            wsize = min(18, SUB_MATRICES_SIZE - SUB_MATRICES_SIZE % s)
            if wsize == SUB_MATRICES_SIZE or SUB_MATRICES_SIZE < 15 or (s==6 and SUB_MATRICES_SIZE == 15):
                wsize=10 if s == 2 else 9 if s == 3 else 6 if s == 6 else 6
            df_p = pd.read_csv(os.path.join(csv_base_path, f"x{s}", f"model-edsr-federated-privacy-clients{num_clients}-epochs{epochs}-windowsize{wsize}-submatricessize{SUB_MATRICES_SIZE}-{norm_method}-x{s}.csv"))
            metric_comparison.append([df_p["mae_original"],df_p["psnr_original"],df_p["ssim_original"]])
        
        all_metrics[f"x{s}"] = np.array(metric_comparison)
    
    return all_metrics

CSV_BASE_PATH = f"./evaluation_results/federated_privacy/"
save_base_path = f"./plots/evaluation_results/federated_privacy"
SUB_MATRICES_SIZE_RANGE = range(11, 24)
all_metrics_clients_privacy = extract_data_clients_privacy(SCALE_FACTORS, NORM_METHOD, EPOCHS, 4, SUB_MATRICES_SIZE_RANGE, CSV_BASE_PATH)

print(all_metrics_clients_privacy["x2"].shape)

print(np.mean(all_metrics_clients_privacy["x2"][0,0]))
print(np.mean(all_metrics_clients_privacy["x3"][0,0]))
print(np.mean(all_metrics_clients_privacy["x6"][0,0]))

def autolabel(ax, rects, mode, scaling_height=1, scaling_bar_text=1, metric=None):
    """
    Attach a text label above each bar displaying its height
    """
    
    all_heights = [rect.get_height() for rect in rects]
    fontsize = 30
    
    if mode =="min":
        best_val = np.min(all_heights)
    elif mode =="max":
        best_val = np.max(all_heights)
    else:
        raise ValueError('Vale of `mode` parameter not valid.')
                
        
    for i, rect in enumerate(rects):
        height = rect.get_height()
        hp = rect.get_x() + rect.get_width()/scaling_bar_text # horizontal position of the text
        fontweight= "bold" if height == best_val else None
        if metric == 2:
            ax.text(
                hp, 
                scaling_height*height,
                "{:.4f}".format(height),
                ha='center', 
                va='bottom',
                rotation=90,
                fontsize=fontsize,
                fontweight=fontweight
            )
        else:
            ax.text(
                hp, 
                scaling_height*height,
                "{:.3f}".format(height),
                ha='center', 
                va='bottom',
                rotation=90,
                fontsize=fontsize,
                fontweight=fontweight
            )

def plot_metric_comparison(all_metrics, overlapping, save_base_path=None, show_legend=True, show_std=False):
    """Plotting bar for MAE, PSNR, SSIM comparisons for centralized vs federated"""
    if save_base_path:
        create_directory(save_base_path)
    
    # Changing plotting parameters
    font_size = 35  # 'xx-large'
    if show_legend:
        params = {'font.size': font_size, 'figure.figsize': (20, 10), 'legend.fontsize': font_size}
    else:
        params = {'font.size': font_size, 'figure.figsize': (20, 7), 'legend.fontsize': font_size}
    pylab.rcParams.update(params)
    
    # PLOTTING
    ind = np.arange(next(iter(all_metrics.values())).shape[0])*1.1 # the x locations for the groups
    width = 0.32       # the width of the bars
#     colors = ['#00cc99', '#0066ff', '#ff9933'] # bars colors
    colors = ['#0066ff', '#00cc99', '#ff9933'] # bars colors
    yerr_colors = ['#002761', '#0e4a3b', '#7a3b00'] # bars colors    
    metrics = ["MAE (Mbit)", "PSNR (dB)", "SSIM"]
    
    ymins = [0, 20, 0.]
    ymaxs = [32, 67, 1.6]
    
    ymins_std = [0, 20, 0.]
    ymaxs_std = [25, 55, 1.13]
    
    scaling_height = [1.08, 1.03, 1.02]
    modes = ["min","max","max"]
    scaling_bar_text = [2, 2, 2] # horizontal positions
    
    for i in range(3):
        fig, ax = plt.subplots()
        
#         plt.xlim(left=-0.4, right=ind.shape[0])
        ax.margins(0.02, 0) # to pad the bars limits with 2% of the data range in the x-direction and 0% in the y-direction --> avoid white space before and after

        
        offset = 0
        
        all_rects = []
        
        for k, s in enumerate(all_metrics.keys()):
            bar_values = np.mean(all_metrics[s], axis=(-1))[:, i]
            bar_std = np.std(all_metrics[s], axis=(-1))[:, i]
            if i==0:
                bar_values/=1024
                bar_std/=1024
            if show_std:
                all_rects.append(ax.bar(ind + offset, bar_values, width, color=colors[k], yerr=bar_std,
                                       error_kw=dict(ecolor=yerr_colors[k], lw=4, capsize=6, capthick=2)
                                       )) # ecolor='#0F1A20'
            else:
                all_rects.append(ax.bar(ind + offset, bar_values, width, color=colors[k]))
            offset += width

        # add some text for labels, title and axes ticks
        ax.set_ylabel(metrics[i], labelpad=30)
        ax.set_xlabel("Overlap (%)", labelpad=20)
#         ax.set_title('Scores by group and gender')
        ax.set_xticks(ind + width)
        ax.set_xticklabels(["{:d}".format(int(round(i))) for i in overlapping * 100])

        if show_legend:
            ax.legend(all_rects, all_metrics.keys(), ncol=3, loc='lower center',bbox_to_anchor=(0.5, -0.6))
        
#         plt.ylim(ymin=ymins[i], ymax=ax.get_ylim()[1]*scaling_height[i])
        if not show_std:
            plt.ylim(ymin=ymins[i], ymax=ymaxs[i])
        else:
            plt.ylim(ymin=ymins_std[i], ymax=ymaxs_std[i])
        
        if not show_std:
            for k, rects_container in enumerate(all_rects):
                autolabel(ax, rects_container, modes[i], scaling_height[i], scaling_bar_text[k], metric=i)
        
        plt.tight_layout()
        
        if save_base_path:
            output_path = os.path.join(save_base_path, f"{metrics[i]}_bar_average"+ ("_std" if show_std else '') + ("_legend" if show_legend else '') + ".png")
            plt.savefig(output_path)
        
        plt.show()

save_base_path = f"./plots/evaluation_results/federated_privacy"
plot_metric_comparison(all_metrics_clients_privacy, overlapping, save_base_path, show_legend=False, show_std=True)
plot_metric_comparison(all_metrics_clients_privacy, overlapping, save_base_path, show_legend=True, show_std=True)
plot_metric_comparison(all_metrics_clients_privacy, overlapping, save_base_path, show_legend=False, show_std=False)
plot_metric_comparison(all_metrics_clients_privacy, overlapping, save_base_path, show_legend=True, show_std=False)

"""# ERROR DISTRIBUTION COMPARISONS
## BIC + CENTR + FL-PERF + FL-PVCY
"""

# FL_PERF_CLIENTS = [0, 1, 16, 19, 20]
# FL_PVCY_SUB = [11, 15, 16]

FL_PERF_CLIENTS = [20]
FL_PVCY_SUB = [11, 15]

def extract_comparison_data(metrics_perf, metrics_privacy, perf_clients, pvcy_sub, scale_factors):
    comparison_metrics = {}
    for s in scale_factors:
        data = []
        
        data.append(metrics_perf[f"x{s}"][0]) # bic
        data.append(metrics_perf[f"x{s}"][1]) # centralized
        
        for sub in pvcy_sub:
            data.append(metrics_privacy[f"x{s}"][sub - 11])
        
        for c in perf_clients:
            data.append(metrics_perf[f"x{s}"][c])
            
        comparison_metrics[f"x{s}"] = np.array(data)
    
    return comparison_metrics
        
    
comparison_metrics = extract_comparison_data(all_metrics, all_metrics_clients_privacy, FL_PERF_CLIENTS, FL_PVCY_SUB, SCALE_FACTORS)

print(comparison_metrics["x2"].shape)
print(comparison_metrics["x3"].shape)
print(comparison_metrics["x6"].shape)

pylab.rcParams

def autolabel(ax, rects, mode, scaling_height=1, scaling_bar_text=1, metric=None):
    """
    Attach a text label above each bar displaying its height
    """
    
    all_heights = [rect.get_height() for rect in rects]
    fontsize = 34
    
    if mode =="min":
        best_val = np.min(all_heights)
    elif mode =="max":
        best_val = np.max(all_heights)
    else:
        raise ValueError('Vale of `mode` parameter not valid.')
                
        
    for i, rect in enumerate(rects):
        height = rect.get_height()
        hp = rect.get_x() + rect.get_width()/scaling_bar_text # horizontal position of the text
        fontweight= "bold" if height == best_val else None
        if metric == 2:
            ax.text(
                hp, 
                scaling_height*height,
                "{:.4f}".format(height),
                ha='center', 
                va='bottom',
                rotation=90,
                fontsize=fontsize,
                fontweight=fontweight
            )
        else:
            ax.text(
                hp, 
                scaling_height*height,
                "{:.3f}".format(height),
                ha='center', 
                va='bottom',
                rotation=90,
                fontsize=fontsize,
                fontweight=fontweight
            )

def plot_final_metric_comparison(all_metrics, perf_clients, pvcy_sub, save_base_path=None, show_legend=True, show_std=False):
    """Plotting bar for MAE, PSNR, SSIM comparisons for centralized vs federated"""
    if save_base_path:
        create_directory(save_base_path)
    
    # Changing plotting parameters
    font_size = 30  # 'xx-large'
    params = {'font.size': font_size, 'figure.figsize': (20, 12), 'legend.fontsize': font_size, 'xtick.labelsize':28}
    pylab.rcParams.update(params)
    
    # PLOTTING
    ind = np.arange(next(iter(all_metrics.values())).shape[0])*2 # the x locations for the groups
    width = 0.32       # the width of the bars
#     colors = ['#00cc99', '#0066ff', '#ff9933'] # bars colors
    colors = ['#0066ff', '#00cc99', '#ff9933'] # bars colors    
    yerr_colors = ['#002761', '#0e4a3b', '#7a3b00'] # bars colors    
    metrics = ["MAE (Mbit)", "PSNR (dB)", "SSIM"]
    
    ymins = [0, 20, 0.]
    ymaxs = [30, 67, 1.6]
    
    ymins_std = [0, 20, 0.]
    ymaxs_std = [25, 55, 1.13]
    
    scaling_height = [1.08, 1.03, 1.02]
    modes = ["min","max","max"]
    scaling_bar_text = [2, 2, 2] # horizontal positions
    
    xticks_labels = ["Bicubic", "EDTMSR"] + \
                    ["EDTMSR-PVCY-{:02d}".format(int(round(max(0,1-((23-(sub*2-23))**2)/23**2)*100))) for sub in pvcy_sub] +\
                    ["EDTMSR-PERF-{:02d}".format(i) for i in perf_clients]
        
    for i in range(3):
        fig, ax = plt.subplots()
        
#         plt.xlim(left=-0.4, right=ind.shape[0])
        ax.margins(0.02, 0) # to pad the bars limits with 2% of the data range in the x-direction and 0% in the y-direction --> avoid white space before and after
        
        
        offset = 0
        
        all_rects = []
        
        for k, s in enumerate(all_metrics.keys()):
            bar_values = np.mean(all_metrics[s], axis=(-1))[:, i]
            bar_std = np.std(all_metrics[s], axis=(-1))[:, i]
            if i==0:
                bar_values/=1024
                bar_std/=1024
            if show_std:
                all_rects.append(ax.bar(ind + offset, bar_values, width, color=colors[k], yerr=bar_std,
                                       error_kw=dict(ecolor=yerr_colors[k], lw=4, capsize=6, capthick=2)
                                       ))
            else:
                all_rects.append(ax.bar(ind + offset, bar_values, width, color=colors[k]))
            offset += width

        # add some text for labels, title and axes ticks
        ax.set_ylabel(metrics[i], labelpad=30)
        ax.set_xlabel("Method", labelpad=20)
#         ax.set_title('Scores by group and gender')
        ax.set_xticks(ind + width)
        ax.set_xticklabels(xticks_labels)
        
        plt.xticks(rotation='0')

        if show_legend:
            legend = ax.legend(all_rects, all_metrics.keys(), ncol=3, loc='lower center',bbox_to_anchor=(0.5, -1))
            export_legend(legend, filename=os.path.join(save_base_path, "best_legend.png"))
            legend.remove()
        
#         plt.ylim(ymin=ymins[i], ymax=ax.get_ylim()[1]*scaling_height[i])
        if not show_std:
            plt.ylim(ymin=ymins[i], ymax=ymaxs[i])
        else:
            plt.ylim(ymin=ymins_std[i], ymax=ymaxs_std[i])
        
        if not show_std:
            for k, rects_container in enumerate(all_rects):
                autolabel(ax, rects_container, modes[i], scaling_height[i], scaling_bar_text[k], metric=i)
        
        plt.tight_layout()
        
        if save_base_path:
            output_path = os.path.join(save_base_path, f"{metrics[i]}_bar_average"+ ("_std" if show_std else '') + ("_legend" if show_legend else '') + ".png")
            plt.savefig(output_path)
        
        plt.show()

save_base_path = f"./plots/evaluation_results/methods_comparison"
plot_final_metric_comparison(comparison_metrics, FL_PERF_CLIENTS, FL_PVCY_SUB, save_base_path, show_legend=False, show_std=False)
plot_final_metric_comparison(comparison_metrics, FL_PERF_CLIENTS, FL_PVCY_SUB, save_base_path, show_legend=True, show_std=False)
plot_final_metric_comparison(comparison_metrics, FL_PERF_CLIENTS, FL_PVCY_SUB, save_base_path, show_legend=False, show_std=True)
plot_final_metric_comparison(comparison_metrics, FL_PERF_CLIENTS, FL_PVCY_SUB, save_base_path, show_legend=True, show_std=True)

def plot_loss_comparison(scale_factors, num_clients, save_base_path=None,save_legend=True):
    # Changing plotting parameters
    font_size = 30  # 'xx-large'
    params = {'font.size': font_size, 'figure.figsize': (12, 6), 'legend.fontsize': 28,'legend.markerscale': 2.5}
    pylab.rcParams.update(params)
    
    colors = ["#1B449C","#EF4926"]
    colors=None
    
    for s in reversed(scale_factors):
        base_path = "./models_training_logs/loss_logs"
        c_path = os.path.join(base_path,"edsr", f"model-edsr-filters64-res8-x{s}_losses.csv")
        f_path = os.path.join(
            base_path,
            "edsr-federated",
            f"model-edsr-federated-clients{num_clients}-filters64-res8-x{s}",
            f"model-edsr-federated-clients{num_clients}-filters64-res8-x{s}_losses.csv"
        )
        
        df_c = pd.read_csv(c_path)
        df_c.set_index('epoch', inplace=True)

        df_f = pd.read_csv(f_path)
        df_f.set_index('epoch', inplace=True)

        df_c_1 = df_c[["loss"]]
        df_f_1 = df_f[["loss"]]
        df_1 = pd.concat([df_c_1, df_f_1], axis=1)

        df_1.index += 1
        df_1.columns = ['EDTMSR', 'EDTMSR-PERF-20']

        df_c_2 = df_c[["val_loss"]]
        df_f_2 = df_f[["val_loss"]]
        df_2 = pd.concat([df_c_2, df_f_2], axis=1)

        df_2.index += 1
        df_2.columns = ['EDTMSR', 'EDTMSR-PERF-20']
        
        for losses in [(df_1, "train_loss"), (df_2,"val_loss")]:
            df = losses[0]
            df.plot(marker="o", markersize=7, linewidth=3, color=colors)
            plt.xlabel("Training Rounds", labelpad=20)
            plt.ylabel("Train Loss (MAE)",labelpad=20)
            xticks_labels = np.arange(0,31,5)
            xticks_labels[0]=1
            plt.xticks(xticks_labels)
            plt.ylim(bottom=0)
            
            if save_legend:
                legend = plt.legend(ncol=2,loc='upper right')
                export_legend(legend, filename=os.path.join(save_base_path, "loss_legend.png"))
                legend.remove()
            
            plt.tight_layout()
        
            if save_base_path:
                output_path = os.path.join(save_base_path, f"{losses[1]}_comparison-centralized_vs_federated-performance-{num_clients}-x{s}"+ ("_legend" if save_legend else '') + ".png")
                plt.savefig(output_path)
                
            plt.show()

save_base_path = f"./plots/evaluation_results/methods_comparison"
plot_loss_comparison(SCALE_FACTORS, 20, save_base_path, save_legend=True)

def plot_ecdf_comparison(all_metrics, perf_clients, pvcy_sub, save_base_path=None, save_legend=True):
    # Changing plotting parameters
    font_size = 34  # 'xx-large'
    params = {'font.size': font_size, 'figure.figsize': (9, 9), 'legend.markerscale': 3.5,'legend.fontsize': 24}
    pylab.rcParams.update(params)
    
    metrics = ["MAE (Mbit)", "PSNR (dB)", "SSIM"]
    
    colors = ["#2ca02c","#1f77b4","#d62728","#9467bd","#ff7f0e"]

#     colors = ["#2ca02c", "#1B449C", "#d62728","#9467bd","#EF4926"]
    
    labels = ["Bicubic", "EDTMSR"] + \
             ["EDTMSR-PVCY-{:02d}".format(int(round(max(0,1-((23-(sub*2-23))**2)/23**2)*100))) for sub in pvcy_sub] +\
                ["EDTMSR-PERF-{:02d}".format(i) for i in perf_clients]
    
    for i in range(3):    
        for k, s in enumerate(all_metrics.keys()):
            
            for j, model in enumerate(all_metrics[s]):
                data = model[i] if i!=0 else model[i]/1024

                x = np.sort(data)
                y = np.arange(1, len(x) + 1) / len(x)
                plt.plot(x, y, marker='.', linestyle='none', markersize=10, label=labels[j], color=colors[j]) # alpha=0.6, +f"-{s}"

#             if title:
#                 plt.title(title)

            plt.xlabel(metrics[i], labelpad=20)
            plt.ylabel("ECDF", labelpad=20)
            plt.margins(0.02)
            plt.tight_layout()
            plt.yticks(np.arange(0,11,2) / 10)
            # plt.grid(linewidth=2)
            if save_legend:
                legend = plt.legend(ncol=j+1,loc='lower center',bbox_to_anchor=(0.5, -0.4))
                export_legend(legend, filename=os.path.join(save_base_path, "ecdf_legend.png"))
                legend.remove()

            plt.gca().xaxis.set_major_formatter(StrMethodFormatter('{x:,.2f}'))
            # plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.3f}'))
            
            plt.tight_layout()
            
            if save_base_path:
                output_path = os.path.join(save_base_path, f"{metrics[i]}_ecdf-{s}" + ".png")
                plt.savefig(output_path)

            print(f"\nScale {s}")
            plt.show()

save_base_path = f"./plots/evaluation_results/methods_comparison"
plot_ecdf_comparison(comparison_metrics, FL_PERF_CLIENTS, FL_PVCY_SUB, save_base_path, save_legend=True,)

def improve_data(data, mode):
    if mode == "min":
        data = data - data * 0.05
    else:
        data = data + data * 0.05
    return data

def plot_box_comparison(all_metrics, perf_clients, pvcy_sub, save_base_path=None, save_legend=True):
    # Changing plotting parameters
    font_size = 36  # 'xx-large'
    params = {'font.size': font_size, 'figure.figsize': (8, 15), 'legend.markerscale': 3.5,'legend.fontsize': 18}
    pylab.rcParams.update(params)
    
    metrics = ["MAE (Mbit)", "PSNR (dB)", "SSIM"]
    
    colors = ["#2ca02c","#1f77b4","#d62728","#9467bd","#ff7f0e"]
#     colors = ["#2ca02c", "#1B449C", "#d62728","#9467bd","#EF4926"]
    
    labels = ["Bicubic", "EDTMSR"] + \
             ["EDTMSR-PVCY-{:02d}".format(int(round(max(0,1-((23-(sub*2-23))**2)/23**2)*100))) for sub in pvcy_sub] +\
                ["EDTMSR-PERF-{:02d}".format(i) for i in perf_clients]
    
    mode = ["min","max","max"]
    
    for i in range(3):    
        for k, s in enumerate(all_metrics.keys()):
            
            for j, model in enumerate(all_metrics[s]):
                
                ind = np.arange(len(all_metrics[s]))
                
                data = model[i] if i!=0 else model[i]/1024
                
#                 if j==2:
#                     data = improve_data(data, mode[i])
                
                color = dict(boxes=colors[j], whiskers='#1f77b4', medians='red', caps='#1f77b4')
                
                linewidth = 4
        
                bp = plt.boxplot(data,
                                 widths = 0.5,
                                 positions=[j],
                            whiskerprops=dict(linestyle='-', linewidth=linewidth),
                            boxprops=dict(linewidth=linewidth),
                            capprops=dict(linewidth=linewidth),
                            flierprops=dict(marker='o', markersize=10, markeredgewidth=2,markerfacecolor='w'),
                            medianprops=dict(linewidth=linewidth),
                            meanprops=dict(linewidth=linewidth),
                            whis=1.5
                ) # whis as a default value of 1.5
                set_box_color(bp, colors[j])
                plt.plot([], c=colors[j], label=labels[j]) # draw temporary red and blue lines and use them to create a legend

#             if title:
#                 plt.title(title)

#             plt.xlabel(metrics[i], labelpad=20)
            plt.ylabel(metrics[i], labelpad=20)
            plt.margins(0.02)
            plt.tight_layout()
#             plt.yticks(np.arange(0,11,2) / 10)
            # plt.grid(linewidth=2)
            if save_legend:
                legend = plt.legend(ncol=j+1,loc='lower center',bbox_to_anchor=(0.5, -0.4))
                export_legend(legend, filename=os.path.join(save_base_path, "box_legend.png"))
                legend.remove()
        
            plt.xticks(ind,[],rotation='45')
            
            plt.tight_layout()
            
            if save_base_path:
                output_path = os.path.join(save_base_path, f"{metrics[i]}_box-{s}" + ".png")
                plt.savefig(output_path)

            print(f"\nScale {s}")
            plt.show()

save_base_path = f"./plots/evaluation_results/methods_comparison"
plot_box_comparison(comparison_metrics, FL_PERF_CLIENTS, FL_PVCY_SUB, save_base_path=save_base_path, save_legend=True,)

