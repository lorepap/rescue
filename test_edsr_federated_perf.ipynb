{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import numpy as np\n",
    "from models.edsr import edsr\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_clipping(data, percentage, mode=\"max\"):\n",
    "    quantile_val = np.quantile(data, percentage)\n",
    "    if mode == \"max\":\n",
    "        data = data.clip(max=quantile_val)\n",
    "    if mode == \"min\":\n",
    "        data = data.clip(min=quantile_val)\n",
    "    return data\n",
    "\n",
    "def exp_root_norm(data, exp=2):\n",
    "    return data ** (1 / exp)\n",
    "\n",
    "def minmax_scale(images):\n",
    "    # Assuming images is a 4D array with shape (N, 32, 32)\n",
    "    min_val = np.min(images)\n",
    "    max_val = np.max(images)\n",
    "    \n",
    "    scaled_images = (images - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return scaled_images\n",
    "\n",
    "def preprocess(images):\n",
    "    images = quantile_clipping(images, 0.95, mode=\"max\")\n",
    "    images = exp_root_norm(images, exp=2)\n",
    "    images = minmax_scale(images)\n",
    "    return images\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client_data(fine_matrices, coarse_matrices, num_clients, scale_factor):\n",
    "    print(f\"Distributing different full-size matrices across {num_clients} clients...\")\n",
    "    \n",
    "    num_samples = fine_matrices.shape[0]\n",
    "    samples_per_client = num_samples // num_clients\n",
    "\n",
    "    fine_size = fine_matrices[0].shape[0]\n",
    "    coarse_size = coarse_matrices[0].shape[0]\n",
    "\n",
    "    if coarse_size*scale_factor != fine_size:\n",
    "        print(f\"Size mismatch.. adjusting: {fine_size} -> {coarse_size*scale_factor}\")\n",
    "        fine_size = coarse_size*scale_factor\n",
    "    \n",
    "    # Let's modify the fine matrices (we select filter only fine_size * fine_size dimension)\n",
    "    fine_matrices = [fine[:fine_size, :fine_size] for fine in fine_matrices]\n",
    "    fine_matrices = np.array(fine_matrices).reshape((-1, fine_size, fine_size, 1))\n",
    "\n",
    "    client_data = []\n",
    "    for i in range(num_clients):\n",
    "        start_idx = i * samples_per_client\n",
    "        end_idx = start_idx + samples_per_client if i < num_clients - 1 else num_samples\n",
    "        \n",
    "        client_fine = fine_matrices[start_idx:end_idx]\n",
    "        client_coarse = coarse_matrices[start_idx:end_idx]\n",
    "        \n",
    "        client_data.append((client_coarse, client_fine))\n",
    "    \n",
    "    return client_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "dataset = 'geant'\n",
    "if dataset == 'geant':\n",
    "    original_size = 22\n",
    "elif dataset == 'germany':\n",
    "    original_size = 161\n",
    "\n",
    "scale_factor = 2\n",
    "num_clients = 2\n",
    "path_to_data = 'CNSM/data'\n",
    "ground_truth = f'{dataset}_original_{original_size}.npy'\n",
    "NUM_ROUNDS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CNSM/data/geant_coarse_11_x2.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Load and preprocess your data\u001b[39;00m\n\u001b[1;32m     29\u001b[0m train_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_coarse_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mscale_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_x\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscale_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 30\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     31\u001b[0m train_ground_truth \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_to_data, ground_truth))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     33\u001b[0m train_set \u001b[38;5;241m=\u001b[39m train_set\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, original_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mscale_factor, original_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mscale_factor, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/cnsm/lib/python3.10/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CNSM/data/geant_coarse_11_x2.npy'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for scale_factor in [2, 3, 4]:\n",
    "    for num_clients in range(2, 21):\n",
    "\n",
    "        # Define the TFF model\n",
    "        def model_fn():\n",
    "            keras_model = edsr(input_depth=1, scale=scale_factor, num_filters=64, num_res_blocks=8)\n",
    "            return tff.learning.models.from_keras_model(\n",
    "                keras_model,\n",
    "                input_spec=(\n",
    "                    tf.TensorSpec(shape=(None, coarse_matrix_size, coarse_matrix_size, 1), dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=(None, fine_matrix_size, fine_matrix_size, 1), dtype=tf.float32)\n",
    "                ),\n",
    "                loss=tf.keras.losses.MeanSquaredError(),\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()]\n",
    "            )\n",
    "\n",
    "        def create_tf_dataset_for_client(client_data):\n",
    "            def batch_format_fn(x, y):\n",
    "                return (x, y)\n",
    "            dataset = tf.data.Dataset.from_tensor_slices(client_data)\n",
    "            dataset = dataset.shuffle(buffer_size=len(client_data[0]))\n",
    "            dataset = dataset.batch(32)\n",
    "            dataset = dataset.map(batch_format_fn)\n",
    "            return dataset\n",
    "\n",
    "        # Load and preprocess your data\n",
    "        train_file = f'{dataset}_coarse_{original_size//scale_factor}_x{scale_factor}.npy'\n",
    "        train_set = np.load(os.path.join(path_to_data, train_file)).astype(np.float32)\n",
    "        train_ground_truth = np.load(os.path.join(path_to_data, ground_truth)).astype(np.float32)\n",
    "\n",
    "        train_set = train_set.reshape((-1, original_size//scale_factor, original_size//scale_factor, 1))\n",
    "        train_ground_truth = train_ground_truth.reshape((-1, original_size, original_size, 1))\n",
    "\n",
    "        print(f\"Train set shape: {train_set.shape}\", \"Ground truth shape:\", train_ground_truth.shape)\n",
    "\n",
    "        train_set = preprocess(train_set)\n",
    "        train_ground_truth = preprocess(train_ground_truth)\n",
    "\n",
    "        # Create client data\n",
    "        client_data = create_client_data(train_ground_truth, train_set, num_clients, scale_factor)\n",
    "\n",
    "        # Print the shape of the client data\n",
    "        for i, client in enumerate(client_data):\n",
    "            print(f\"Client {i} LR:\", client[0].shape, \"Client {i} HR:\", client[1].shape)\n",
    "\n",
    "        # Shapes of the client data\n",
    "        coarse_matrix_size = client_data[0][0].shape[1]\n",
    "        fine_matrix_size = client_data[0][1].shape[1]\n",
    "\n",
    "        # Convert all clients data to float 32\n",
    "        client_data = [(x.astype(np.float32), y.astype(np.float32)) for x, y in client_data]\n",
    "        federated_train_data = [create_tf_dataset_for_client(cd) for cd in client_data]\n",
    "\n",
    "        # Create the federated learning process\n",
    "        iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
    "            model_fn,\n",
    "            client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "            server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        )\n",
    "\n",
    "        # Initialize the server state\n",
    "        state = iterative_process.initialize()\n",
    "\n",
    "        # Perform federated training\n",
    "        NUM_ROUNDS = 20\n",
    "        start = time.time()\n",
    "        for round_num in range(NUM_ROUNDS):\n",
    "            state, metrics = iterative_process.next(state, federated_train_data)\n",
    "            print(f'Round {round_num}')\n",
    "            print(metrics)\n",
    "\n",
    "        end = time.time()\n",
    "        print(f\"Training time: {end - start}\")\n",
    "        # Save training time\n",
    "        save_pickle(end - start, f'CNSM/fed_data/{dataset}_fed_training_time_{num_clients}_x{scale_factor}_rounds_{NUM_ROUNDS}.pkl')\n",
    "        # Save the federated weights\n",
    "        fed_weights = state[0].trainable\n",
    "        # Save weights\n",
    "        save_pickle(fed_weights, f'CNSM/fed_data/{dataset}_fed_weights_{num_clients}_x{scale_factor}_rounds_{NUM_ROUNDS}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
