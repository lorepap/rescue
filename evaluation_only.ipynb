{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 17:26:08.065162: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-10 17:26:08.425856: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-10 17:26:08.425900: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-10 17:26:08.425924: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-10 17:26:08.640953: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-10 17:26:08.652671: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-10 17:26:12.275244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-09-10 17:26:23.411038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-10 17:26:24.267935: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import numpy as np\n",
    "from models.edsr import edsr\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_clipping(data, percentage, mode=\"max\"):\n",
    "    quantile_val = np.quantile(data, percentage)\n",
    "    if mode == \"max\":\n",
    "        data = data.clip(max=quantile_val)\n",
    "    if mode == \"min\":\n",
    "        data = data.clip(min=quantile_val)\n",
    "    return data\n",
    "\n",
    "def exp_root_norm(data, exp=2):\n",
    "    return data ** (1 / exp)\n",
    "\n",
    "def minmax_scale(images):\n",
    "    # Assuming images is a 4D array with shape (N, 32, 32)\n",
    "    min_val = np.min(images)\n",
    "    max_val = np.max(images)\n",
    "    \n",
    "    scaled_images = (images - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return scaled_images\n",
    "\n",
    "def preprocess(images):\n",
    "    images = quantile_clipping(images, 0.95, mode=\"max\")\n",
    "    images = exp_root_norm(images, exp=2)\n",
    "    images = minmax_scale(images)\n",
    "    return images\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# def pad_to_nearest_square(matrix, num_clients):\n",
    "#     \"\"\"\n",
    "#     Pad the matrix to the nearest size that's perfectly divisible by sqrt(num_clients).\n",
    "#     \"\"\"\n",
    "#     orig_height, orig_width = matrix.shape[1:3]\n",
    "#     sqrt_clients = int(math.sqrt(num_clients))\n",
    "    \n",
    "#     target_size = math.ceil(max(orig_height, orig_width) / sqrt_clients) * sqrt_clients\n",
    "    \n",
    "#     padded = np.pad(\n",
    "#         matrix,\n",
    "#         ((0, 0), (0, target_size - orig_height), (0, target_size - orig_width), (0, 0)),\n",
    "#         mode='constant'\n",
    "#     )\n",
    "    \n",
    "#     return padded, (orig_height, orig_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'germany'\n",
    "\n",
    "if dataset == 'geant':\n",
    "    original_size = 22\n",
    "elif dataset == 'germany':\n",
    "    original_size = 161\n",
    "\n",
    "NUM_ROUNDS = 20\n",
    "overlap_perc = 5\n",
    "num_clients = 4\n",
    "c_scale_factor = 2  # This determines the downsampling factor for creating coarse-grained matrices for each client\n",
    "scale_factor = 2\n",
    "path_to_data = 'CNSM/data'\n",
    "ground_truth = f'{dataset}_original_{original_size}.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(y_true, y_pred, max_value=1.0):\n",
    "    \"\"\"\n",
    "    Calculate Peak Signal-to-Noise Ratio (PSNR)\n",
    "    \"\"\"\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * np.log10(max_value / np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (8993, 80, 80, 1) Ground truth shape: (8993, 161, 161, 1)\n",
      "57/57 [==============================] - 65s 1s/step\n",
      "57/57 [==============================] - 63s 1s/step\n",
      "Mean Squared Error (MSE): 0.0710\n",
      "Root Mean Squared Error (RMSE): 0.2664\n",
      "Mean Absolute Error (MAE): 0.1221\n",
      "Mean Absolute Percentage Error: 97.6102\n",
      "WMean Absolute Error (WMAE): 0.6530\n",
      "Average Structural Similarity Index (SSIM): 0.0693\n",
      "Average Peak Signal-to-Noise Ratio (PSNR): 12.16 dB\n",
      "Train set shape: (8993, 80, 80, 1) Ground truth shape: (8993, 161, 161, 1)\n",
      "57/57 [==============================] - 65s 1s/step\n",
      "57/57 [==============================] - 65s 1s/step\n",
      "Mean Squared Error (MSE): 0.0708\n",
      "Root Mean Squared Error (RMSE): 0.2660\n",
      "Mean Absolute Error (MAE): 0.1234\n",
      "Mean Absolute Percentage Error: 95.9937\n",
      "WMean Absolute Error (WMAE): 0.6511\n",
      "Average Structural Similarity Index (SSIM): 0.0417\n",
      "Average Peak Signal-to-Noise Ratio (PSNR): 12.17 dB\n",
      "Train set shape: (8993, 80, 80, 1) Ground truth shape: (8993, 161, 161, 1)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CNSM/fed_data/germany_fed_weights_4_x2_rounds_20_overlap_5.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m train_ground_truth \u001b[38;5;241m=\u001b[39m preprocess(train_ground_truth)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Load the federated weights\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m fed_weights \u001b[38;5;241m=\u001b[39m \u001b[43mload_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCNSM/fed_data/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_fed_weights_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mn\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_x2_rounds_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mNUM_ROUNDS\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_overlap_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mp\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Create a new model and load the weights\u001b[39;00m\n\u001b[1;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m edsr(input_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, scale\u001b[38;5;241m=\u001b[39mscale_factor, num_filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_res_blocks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m, in \u001b[0;36mload_pickle\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_pickle\u001b[39m(filename):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/miniconda3/envs/cnsm/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CNSM/fed_data/germany_fed_weights_4_x2_rounds_20_overlap_5.pkl'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "for p in [5, 25, 50, 75]:\n",
    "    for n in [2, 3, 4, 5, 6, 7]:\n",
    "        coarse_size = original_size // scale_factor\n",
    "        fine_size = coarse_size * scale_factor\n",
    "        train_file = f'{dataset}_coarse_{coarse_size}_x{scale_factor}.npy'\n",
    "        train_set = np.load(os.path.join(path_to_data, train_file)).astype(np.float32)\n",
    "        train_ground_truth = np.load(os.path.join(path_to_data, ground_truth)).astype(np.float32)\n",
    "\n",
    "        # node_to_index = np.load(os.path.join(path_to_data, 'node_to_index.npy'), allow_pickle=True).item()\n",
    "\n",
    "        train_set = train_set.reshape((-1, coarse_size, coarse_size, 1))\n",
    "        train_ground_truth = train_ground_truth.reshape((-1, original_size, original_size, 1))\n",
    "        print(f\"Train set shape: {train_set.shape}\", \"Ground truth shape:\", train_ground_truth.shape)\n",
    "\n",
    "        train_set = preprocess(train_set)\n",
    "        train_ground_truth = preprocess(train_ground_truth)\n",
    "\n",
    "        # Load the federated weights (overlap evaluation)\n",
    "        fed_weights = load_pickle(f'CNSM/fed_data/{dataset}_fed_weights_{n}_x2_rounds_{NUM_ROUNDS}_overlap_{p}.pkl')\n",
    "\n",
    "        # Create a new model and load the weights\n",
    "        model = edsr(input_depth=1, scale=scale_factor, num_filters=64, num_res_blocks=8)\n",
    "        model.set_weights(fed_weights)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "        # Split the data into train and test sets\n",
    "        train_size = int(0.8 * len(train_set))\n",
    "        x_train, x_test = train_set[:train_size], train_set[train_size:]\n",
    "        y_train, y_test = train_ground_truth[:train_size], train_ground_truth[train_size:]\n",
    "\n",
    "        # Flatten the arrays to 1D for easier calculation\n",
    "        test_predictions = model.predict(x_test)\n",
    "\n",
    "        # Flatten the arrays to 1D for easier calculation\n",
    "        test_predictions = model.predict(x_test)\n",
    "        if test_predictions.shape[1] != y_test.shape[1]:\n",
    "            y_test = y_test[:, :test_predictions.shape[1], :test_predictions.shape[1]] # To match the model output\n",
    "        y_true = y_test.flatten()\n",
    "        y_pred = test_predictions.flatten()\n",
    "\n",
    "        # Calculate MSE\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "        # Calculate RMSE (Root Mean Squared Error)\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        # Calculate MAE\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "        # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "        epsilon = 1e-10\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            # Calculate absolute percentage error\n",
    "            abs_percent_error = np.abs((y_true - y_pred) / (y_true + epsilon))\n",
    "\n",
    "        # Handle cases where y_true is zero\n",
    "        zero_true_mask = np.abs(y_true) <= epsilon\n",
    "        abs_percent_error[zero_true_mask] = np.where(\n",
    "            np.abs(y_pred[zero_true_mask]) <= epsilon,\n",
    "            0,  # Both true and predicted are zero (or very close)\n",
    "            1  # True is zero, but predicted is non-zero\n",
    "        )\n",
    "        # Calculate the mean of the absolute percentage errors\n",
    "        mape = np.mean(abs_percent_error) * 100\n",
    "\n",
    "\n",
    "        # WMAE\n",
    "        abs_errors = np.abs(y_true - y_pred)\n",
    "        weights = np.log1p(np.abs(y_true) + 1e-6)\n",
    "        weights = weights / np.sum(weights)\n",
    "        weighted_errors = abs_errors * weights\n",
    "        wmae =  np.sum(weighted_errors)\n",
    "        \n",
    "        ssim_scores = []\n",
    "        for i in range(y_test.shape[0]):\n",
    "            ssim_score = ssim(y_test[i, :, :, 0], test_predictions[i, :, :, 0], data_range=1.0)\n",
    "            ssim_scores.append(ssim_score)\n",
    "        avg_ssim = np.mean(ssim_scores)\n",
    "\n",
    "        # Calculate PSNR\n",
    "        psnr_scores = []\n",
    "        for i in range(y_test.shape[0]):\n",
    "            psnr_score = calculate_psnr(y_test[i, :, :, 0], test_predictions[i, :, :, 0])\n",
    "            psnr_scores.append(psnr_score)\n",
    "        avg_psnr = np.mean(psnr_scores)\n",
    "\n",
    "\n",
    "        # Calculate the weighted MAE (WMAE): errors on values close to 0 are less penalized, as they are less important (sparse matrix)\n",
    "\n",
    "        print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "        print(f\"Mean Absolute Percentage Error: {mape:.4f}\")\n",
    "        print(f\"WMean Absolute Error (WMAE): {wmae:.4f}\")\n",
    "        print(f\"Average Structural Similarity Index (SSIM): {avg_ssim:.4f}\")\n",
    "        print(f\"Average Peak Signal-to-Noise Ratio (PSNR): {avg_psnr:.2f} dB\")\n",
    "\n",
    "        # save in a csv file\n",
    "        csv_filename = os.path.join(path_to_data, f\"csv_{dataset}_fed_num_clients_{n}_x{scale_factor}_overlap_{p}.csv\")\n",
    "        with open(csv_filename, \"w\") as f:\n",
    "            f.write(f\"MSE, RMSE, MAE, MAPE, WMAE, SSIM, PSNR\\n\")\n",
    "            f.write(f\"{mse:.4f}, {rmse:.4f}, {mae:.4f}, {mape:.4f}, {wmae:.4f}, {avg_ssim:.4f}, {avg_psnr:.2f}\\n\")\n",
    "            f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
